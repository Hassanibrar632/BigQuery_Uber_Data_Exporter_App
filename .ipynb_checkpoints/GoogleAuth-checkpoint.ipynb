{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.auth.transport.requests import Request\n",
    "import google.auth\n",
    "\n",
    "# Path to your service account key\n",
    "SERVICE_ACCOUNT_FILE = 'ancient-cortex-414811-c1b300442177.json'\n",
    "\n",
    "# Define the required scopes for your use case\n",
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']  # For general Google Cloud access\n",
    "\n",
    "def authenticate_service_account():\n",
    "    \"\"\"Authenticate using a service account and return the credentials.\"\"\"\n",
    "    # Authenticate using the service account credentials\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    \n",
    "    # You can refresh credentials if needed (e.g., if expired)\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    \n",
    "    return creds\n",
    "\n",
    "# Call the authentication function to get the credentials\n",
    "credentials = authenticate_service_account()\n",
    "\n",
    "# Check if authentication was successful by printing the credentials\n",
    "print(f\"Authentication successful! Access token: {credentials.token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to your service account key file (JSON)\n",
    "service_account_key_path = \"ancient-cortex-414811-c1b300442177.json\"\n",
    "\n",
    "# Create credentials from the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_key_path)\n",
    "\n",
    "# Initialize the BigQuery client using the service account credentials\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "print(f\"Using credentials: {credentials}\")\n",
    "print(f\"Project ID: {project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using credentials: <google.oauth2.credentials.Credentials object at 0x0000019FE7831010>\n",
      "Project ID: ancient-cortex-414811\n"
     ]
    }
   ],
   "source": [
    "import google.auth\n",
    "\n",
    "# Check the default credentials and project\n",
    "credentials, project = google.auth.default()\n",
    "\n",
    "# Output the credentials and project ID\n",
    "print(f\"Using credentials: {credentials}\")\n",
    "print(f\"Project ID: {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routine ID: APAC_Triage_Ticket_Data_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Chat_Agent_KPIs_Intra_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Chat_Agent_KPIs_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Chat_Ticket_Solved_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Chat_Ticket_Solved_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Messaging_Ticket_Misroutes_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_APAC_Triage_KPIs_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_APAC_Triage_KPIs_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_Compliance_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_Compliance_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_EMEA_Triage_KPIs_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_EMEA_Triage_KPIs_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_KPIs_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Agent_KPIs_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Chat_Ticket_CSAT_2_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Chat_Ticket_CSAT_2_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Handle_Time_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Reopens_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Reopens_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Skips_Abandons_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Skips_Abandons_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Solved_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Msg_Ticket_Solved_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Phone_Agent_KPIs_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Phone_Ticket_CSAT_2_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Phone_Ticket_CSAT_2_Weekly\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Phone_Ticket_KPIs_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: Bliss_Phone_Ticket_KPIs_Half_Hour_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n",
      "Routine ID: EMEA_Triaged_Tickets_Daily\n",
      "Routine Type: TABLE_VALUED_FUNCTION\n",
      "Routine Language: SQL\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Specify the dataset\n",
    "dataset_id = \"ancient-cortex-414811.bpo_datasharing\"\n",
    "\n",
    "# List routines in the dataset\n",
    "routines = client.list_routines(dataset=dataset_id)\n",
    "\n",
    "for routine in routines:\n",
    "    print(f\"Routine ID: {routine.routine_id}\")\n",
    "    print(f\"Routine Type: {routine.type_}\")\n",
    "    print(f\"Routine Language: {routine.language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for table: Bliss_Msg_Agent_Compliance_Daily in dataset: bpo_datasharing\n",
      "Data for Bliss_Msg_Agent_Compliance_Daily in dataset bpo_datasharing saved to D:/Uber Data/bpo_datasharing_Bliss_Msg_Agent_Compliance_Daily_2024-11-26.csv\n",
      "Query executed successfully for table: Bliss_Phone_Ticket_CSAT_2_Daily in dataset: bpo_datasharing\n",
      "Data for Bliss_Phone_Ticket_CSAT_2_Daily in dataset bpo_datasharing saved to D:/Uber Data/bpo_datasharing_Bliss_Phone_Ticket_CSAT_2_Daily_2024-11-26.csv\n",
      "Query executed successfully for table: Global_Identity_Document_Agent_Active_Audits_Daily in dataset: documents\n",
      "Data for Global_Identity_Document_Agent_Active_Audits_Daily in dataset documents saved to D:/Uber Data/documents_Global_Identity_Document_Agent_Active_Audits_Daily_2024-11-26.csv\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Specify your project\n",
    "project_id = \"ancient-cortex-414811\"\n",
    "\n",
    "# Dictionary to hold datasets and their corresponding tables\n",
    "datasets_and_tables = {\n",
    "    \"bpo_datasharing\": [\n",
    "        \"Bliss_Msg_Agent_Compliance_Daily\",\n",
    "        \"Bliss_Phone_Ticket_CSAT_2_Daily\",\n",
    "        # Add more tables for this dataset\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        \"Global_Identity_Document_Agent_Active_Audits_Daily\"\n",
    "        # Add more tables for this dataset\n",
    "    ]\n",
    "    # Add more datasets and their tables here\n",
    "}\n",
    "\n",
    "# Query parameters\n",
    "start_date = \"2024-11-20\"  # Replace with your start date\n",
    "end_date = \"2024-11-20\"    # Replace with your end date\n",
    "\n",
    "# Get the current date to append to filenames\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Loop through each dataset and its tables\n",
    "for dataset_id, tables in datasets_and_tables.items():\n",
    "    for table in tables:\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{project_id}.{dataset_id}.{table}`(@start_date, @end_date)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Configure query parameters\n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            query_parameters=[\n",
    "                bigquery.ScalarQueryParameter(\"start_date\", \"STRING\", start_date),\n",
    "                bigquery.ScalarQueryParameter(\"end_date\", \"STRING\", end_date),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Execute the query\n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            result = query_job.result()\n",
    "            \n",
    "            print(f\"Query executed successfully for table: {table} in dataset: {dataset_id}\")\n",
    "            \n",
    "            # Save each table's data to a CSV file with the date appended\n",
    "            output_file = f\"D:/Uber Data/{dataset_id}_{table}_{start_date}.csv\"  # File name with dataset and date\n",
    "            with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                \n",
    "                # Write the header row\n",
    "                writer.writerow([field.name for field in result.schema])\n",
    "                \n",
    "                # Write the rows\n",
    "                for row in result:\n",
    "                    writer.writerow(row.values())\n",
    "            \n",
    "            print(f\"Data for {table} in dataset {dataset_id} saved to {output_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {table} in dataset {dataset_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "def authenticate_google_drive():\n",
    "    \"\"\"\n",
    "    Authenticate with Google Drive using credentials.json file.\n",
    "    \"\"\"\n",
    "    # Ensure you have 'token.json' for saved sessions; if not, it will prompt for authentication\n",
    "    credentials = Credentials.from_authorized_user_file(\n",
    "        'credentials.json', scopes=[\"https://www.googleapis.com/auth/drive.file\"]\n",
    "    )\n",
    "    return build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "def upload_file_to_google_drive(service, local_file_path, drive_folder_id=None):\n",
    "    \"\"\"\n",
    "    Upload a single file to Google Drive.\n",
    "    \n",
    "    Args:\n",
    "        service: Authenticated Google Drive service instance.\n",
    "        local_file_path: Path to the local file to upload.\n",
    "        drive_folder_id: (Optional) ID of the Google Drive folder to upload the file to.\n",
    "    \"\"\"\n",
    "    file_metadata = {\n",
    "        'name': os.path.basename(local_file_path),  # Use file name from the path\n",
    "        'parents': [drive_folder_id] if drive_folder_id else []  # Add folder if specified\n",
    "    }\n",
    "    media = MediaFileUpload(local_file_path, resumable=True)\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f\"Uploaded file {local_file_path} with ID: {file.get('id')}\")\n",
    "\n",
    "def upload_multiple_files(local_directory, drive_folder_id=None):\n",
    "    \"\"\"\n",
    "    Upload all files from the specified local directory to Google Drive.\n",
    "    \n",
    "    Args:\n",
    "        local_directory: Path to the local directory containing files to upload.\n",
    "        drive_folder_id: (Optional) ID of the Google Drive folder to upload the files to.\n",
    "    \"\"\"\n",
    "    # Get a list of all files in the specified directory\n",
    "    for filename in os.listdir(local_directory):\n",
    "        local_file_path = os.path.join(local_directory, filename)\n",
    "        # Only upload if it's a file (skip subdirectories)\n",
    "        if os.path.isfile(local_file_path):\n",
    "            upload_file_to_google_drive(drive_service, local_file_path, drive_folder_id)\n",
    "\n",
    "# Authenticate and initialize the Google Drive service\n",
    "drive_service = authenticate_google_drive()\n",
    "\n",
    "# Path to the local directory containing files to upload\n",
    "local_directory = \"D:/Uber Data\"  # Replace with your local directory path\n",
    "\n",
    "# Optional: Specify the Google Drive folder ID (or leave it None for the root folder)\n",
    "drive_folder_id = None  # Replace with your folder ID if needed\n",
    "\n",
    "# Upload all files from the local directory to Google Drive\n",
    "upload_multiple_files(local_directory, drive_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "def authenticate_google_drive():\n",
    "    \"\"\"Shows basic usage of the Google Drive API.\"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json',  SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Call the authenticate function to generate token\n",
    "drive_service = authenticate_google_drive()\n",
    "print(\"Authentication successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "def authenticate_google_drive():\n",
    "    \"\"\"\n",
    "    Authenticate with Google Drive using credentials.json file.\n",
    "    \"\"\"\n",
    "    # Ensure you have 'token.json' for saved sessions; if not, it will prompt for authentication\n",
    "    credentials = Credentials.from_authorized_user_file(\n",
    "        'credentials.json', scopes=[\"https://www.googleapis.com/auth/drive.file\"]\n",
    "    )\n",
    "    return build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "def upload_file_to_google_drive(service, local_file_path, drive_folder_id=None):\n",
    "    \"\"\"\n",
    "    Upload a single file to Google Drive.\n",
    "    \n",
    "    Args:\n",
    "        service: Authenticated Google Drive service instance.\n",
    "        local_file_path: Path to the local file to upload.\n",
    "        drive_folder_id: (Optional) ID of the Google Drive folder to upload the file to.\n",
    "    \"\"\"\n",
    "    file_metadata = {\n",
    "        'name': os.path.basename(local_file_path),  # Use file name from the path\n",
    "        'parents': [drive_folder_id] if drive_folder_id else []  # Add folder if specified\n",
    "    }\n",
    "    media = MediaFileUpload(local_file_path, resumable=True)\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f\"Uploaded file {local_file_path} with ID: {file.get('id')}\")\n",
    "\n",
    "def upload_multiple_files(local_directory, drive_folder_id=None):\n",
    "    \"\"\"\n",
    "    Upload all files from the specified local directory to Google Drive.\n",
    "    \n",
    "    Args:\n",
    "        local_directory: Path to the local directory containing files to upload.\n",
    "        drive_folder_id: (Optional) ID of the Google Drive folder to upload the files to.\n",
    "    \"\"\"\n",
    "    # Get a list of all files in the specified directory\n",
    "    for filename in os.listdir(local_directory):\n",
    "        local_file_path = os.path.join(local_directory, filename)\n",
    "        # Only upload if it's a file (skip subdirectories)\n",
    "        if os.path.isfile(local_file_path):\n",
    "            upload_file_to_google_drive(drive_service, local_file_path, drive_folder_id)\n",
    "\n",
    "# Authenticate and initialize the Google Drive service\n",
    "drive_service = authenticate_google_drive()\n",
    "\n",
    "# Path to the local directory containing files to upload\n",
    "local_directory = \"D:/Uber Data\"  # Replace with your local directory path\n",
    "\n",
    "# Optional: Specify the Google Drive folder ID (or leave it None for the root folder)\n",
    "drive_folder_id = None  # Replace with your folder ID if needed\n",
    "\n",
    "# Upload all files from the local directory to Google Drive\n",
    "upload_multiple_files(local_directory, drive_folder_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
